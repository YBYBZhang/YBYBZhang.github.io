<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yabo Zhang</title>
  
  <meta name="author" content="Yabo Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="images/favicon.ico" type="image/vnd.microsoft.icon" />

	<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yabo Zhang (Âº†‰∫öÂçö)</name>
              </p>
              <p>I am a second-year PhD student supervised by Prof. <a href="https://scholar.google.com/citations?hl=en&user=rUOpCEYAAAAJ&view_op=list_works"> Wangmeng Zuo</a> at <a href="http://www.hit.edu.cn/">Harbin Institute of Technology</a>.
              Prior it, I obtained bachelor and master degrees in computer science at the same university in 2021 and 2023, respectively.
              </p>
              <p>My research interests focus on image/video synthesis, multimodal language models.</p>
              
              <p style="text-align:center">
                <a href="mailto:hitzhangyabo2017@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/YaboZhang-CV.pdf">CV (Updated in 2025.01)</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=LnYDPdAAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/YBYBZhang/">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/%E4%BA%9A%E5%8D%9A-%E5%BC%A0-30907a1b1/">Linkedin</a>
              </p>
            </td>
            <td style="padding:2.5%;width:25%;max-width:25%">
              <a href="images/YaboZhang.jpg"><img style="width:125%;max-width:125%" alt="profile photo" src="images/YaboZhang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
              <!-- Representative papers are <span class="highlight">highlighted</span>. -->
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" style="display:flex; align-items:center;">
                <img src='images/FramePainter.png' width="140">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>FramePainter: Endowing Interactive Image Editing with Video Diffusion Priors</papertitle>
              <br>
              <strong>Yabo Zhang</strong>, Xinpeng Zhou, Yihan Zeng, Hang Xu, Wangmeng Zuo
              <br>
              <em>arXiv</em> 2025
              <p>
                <a href="https://arxiv.org/abs/2501.08225">Paper</a> |
                <a href="https://github.com/YBYBZhang/FramePainter">Code [370+ starsüåü]</a> |
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle; display: flex; align-items: center; justify-content: center;">
                <div class="one" style="display:flex; align-items:center;">
                  <img src='images/VideoElevator.png' width="220" style="display:block;">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle;">
                <papertitle>VideoElevator: Elevating Video Generation Quality with Versatile Text-to-Image Diffusion Models</papertitle>
                <br>
                <strong>Yabo Zhang</strong>, Yuxiang Wei, Xianhui Lin, Zheng Hui, Peiran Ren, Xuansong Xie, Xiangyang Ji, Wangmeng Zuo
                <br>
                <em>AAAI</em> 2025
                <p>
                  <a href="https://arxiv.org/abs/2403.05438">Paper</a> |
                  <a href="https://github.com/YBYBZhang/VideoElevator">Code [150+ starsüåü]</a> |
                  <a href="https://videoelevator.github.io/">Project Page</a>
                </p>
              </td>
            </tr>
          </tbody>
        </table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/controlvideo.png' width="140">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>ControlVideo: Training-free Controllable Text-to-Video Generation</papertitle>
              <br>
              <strong>Yabo Zhang</strong>, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, Qi Tian
              <br>
              <em>ICLR</em> 2024
              <p>
                <a href="https://arxiv.org/abs/2305.13077">Paper</a> |
                <a href="https://github.com/YBYBZhang/ControlVideo">Code [800+ starsüåü]</a> |
                <a href="https://huggingface.co/spaces/Yabo/ControlVideo">Demo</a>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/VQ-Font.png' width="170">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>VQ-Font: Few-Shot Font Generation with Structure-Aware Enhancement and
                  Quantization</papertitle>
              <br>
              Mingshuai Yao, <strong>Yabo Zhang</strong>, Xianhui Lin, Xiaoming Li, Wangmeng Zuo
              <br>
              <em>AAAI</em> 2024
              <p>
                <a href="https://arxiv.org/pdf/2308.13999">Paper</a> |
                <a href="https://github.com/Yaomingshuai/VQ-Font">Code</a>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/Elite.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation</papertitle>
              <br>
              Yuxiang Wei, <strong>Yabo Zhang</strong>, Zhilong Ji, Jinfeng Bai, Lei Zhang, Wangmeng Zuo
              <br>
              <em>ICCV</em> 2023 <span style="color: red;"><strong>(Oral)</strong></span>
              <p>
                <a href="https://arxiv.org/abs/2302.13848">Paper</a> | 
                <a href="https://github.com/csyxwei/ELITE">Code [500+ starsüåü]</a> | 
                <a href="https://huggingface.co/spaces/ELITE-library/ELITE">Demo</a>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/DiFa.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>DiFa: Towards Diverse and Faithful One-shot Adaption of Generative Adversarial Networks</papertitle>
              <br>
              <strong>Yabo Zhang</strong>, Mingshuai Yao, Yuxiang Wei, Zhilong Ji, Jinfeng Bai, Wangmeng Zuo
              <br>
              <em>NeurIPS</em> 2022
              <p>
                <a href="https://arxiv.org/abs/2207.08736">Paper</a> | 
                <a href="https://github.com/YBYBZhang/DiFa">Code</a> | 
                <a href="https://nips.cc/media/neurips-2022/Slides/54572.pdf">Slides</a> | 
                <a href="https://recorder-v3.slideslive.com/?share=72234&s=9695aa5e-947a-4768-95e0-d1222e296669">Video</a>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/Text_segmentation.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Associating Spatially-Consistent Grouping with Text-supervised
                  Semantic Segmentation</papertitle>
              <br>
              <strong>Yabo Zhang</strong>, Zihao Wang, Jun Hao Liew, Jiashi Feng, Manyu Zhu, Wangmeng Zuo
              <br>
              <em>Arxiv 2023</em>
              <p>
                <a href="https://arxiv.org/abs/2304.01114">Paper</a> |
                <a href="https://ybybzhang.github.io/">Code</a>
              </p>
            </td>
          </tr>
        </tbody></table>
        
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Internships</heading>
        </td>
      </tr>
      </tbody></table>
      <ul>
        <li>
         Bytedance Seed Vision, May.2025 - Now
          <br>
          Mentored by <a href="https://scholar.google.com/citations?user=-N8Y3GEAAAAJ&hl=zh-CN">Xun Wang</a> and <a href="https://scholar.google.com/citations?user=78vU1IUAAAAJ&hl=zh-CN">Weilin Huang</a>.
          <br>
        </li>

        <br>
        <li>
          Noah's Ark Lab, Jan.2025 - May.2025
          <br>
          Mentored by <a href="https://scholar.google.com/citations?user=YiDxCoAAAAAJ&hl=zh-CN">Yihan Zeng</a> and <a href="https://scholar.google.com/citations?user=J_8TX6sAAAAJ&hl=en">Hang Xu</a>.
          <br>
        </li>

        <br>
        <li>
          Bytedance Intelligent Creation, Jun.2022 - Jan.2023
          <br>
          Mentored by <a href="https://scholar.google.com.hk/citations?user=u5cdqewAAAAJ&hl=en">Zihao Wang</a>, work closely with <a href="https://sites.google.com/site/jshfeng/">Jiashi Feng</a> and <a href="https://scholar.google.com.sg/citations?user=8gm-CYYAAAAJ&hl=en">Jun Hao Liew</a>.
          <br>
        </li>

        <br>
        <li>
          Bytedance AILab, Apr. 2021 - Aug. 2021
          <br>
          Focus on table detection and recognition in photo scene.
        </li>
      </ul>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td>
            <heading>Service</heading>
          </td>
        </tr>
      </tbody></table>

      <table width="100%" align="center" border="0" cellpadding="20"><tbody>
      </tbody></table>
      <ul>
        <li>
          Journal reviewers: TPAMI, TIP, TNNLS, TCSVT, Science China, Machine Learning
        </li>
        <li>
          Conference reviewers: NeurIPS (2023, 2024, 2025), ICLR (2024, 2025), CVPR (2024, 2025), ICML (2024, 2025), ECCV 2024, AAAI 2025
        </li>
      </ul>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Honors</heading>
        </td>
        </tr>
      </tbody></table>

			<ul>
        <li>
          China National Scholarship (2022)
        </li>
        <br>
        <li>
          Excellent Graduate at Harbin Institute of Technology (2021, 2023)
        </li>
        <br>
        <li>
          First-Class People's Scholarship (Top 3%)
        </li>
        <br>
        <li>
          Huawei Enterprise Scholarship (Top 3%)
        </li>
        <br>
        <li>
          Meritorious Winner in Mathematical Contest in Modeling (Top 7%)
        </li>
      </ul>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td>
            <heading>Teaching</heading>
          </td>
        </tr>
      </tbody></table>

      <table width="100%" align="center" border="0" cellpadding="20"><tbody>
      </tbody></table>
      <ul>
        <p>
          Teaching assistant in Harbin Institute of Technology.
        </p>
        <li>
          CS32262: Pattern Recognition Deep Learning, Spring 2022
        </li>
        <br>
        <li>
          CS32131: Data Structures and Algorithms, Fall 2021
        </li>
      </ul>
      
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:0px">
            <p style="text-align:right;font-size:small;">
              Page template is take from this <a href="https://github.com/jonbarron/jonbarron_website">template</a>.
            </p>
          </td>
        </tr>
      </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
